\documentclass{article} % For LaTeX2e
\usepackage{iclr2016_conference,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{pgf}
\usepackage{tikz}
\usepackage{amsmath}

\usetikzlibrary{arrows}

\title{Three-Dimensional Attention for Video Classification}

\author{
Tim Cooijmans, Nicolas Ballas, Aaron Courville \\
Universit\'e de Montr\'eal \\
\texttt{\{tim.cooijmans,nicolas.ballas,aaron.courville\}@umontreal.ca}
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\newcommand{\veci}[1]{\mathbf{#1}}
\newcommand{\vecsub}[2]{#1_{\vecsub{#1}}}

%\iclrfinalcopy % Uncomment for camera-ready version

\begin{document}

\maketitle

\begin{abstract}
We introduce a recurrent model for video classification that employs a differentiable spatiotemporal attention mechanism to apply a three-dimensional convolutional neural network to salient regions at varying resolutions. In addition, we introduce a synthetic video classification task that requires aggregation of information over time to solve, which is confirmed by comparison of the performance of three-dimensional and two-dimensional convolutional architectures. We show performance of our model on this dataset as well as on the UCF101 human activity recognition dataset.
\end{abstract}

\section{Introduction}

\section{Related work}

\section{Model description}

Our model processes a video $V$ through a sequence of three-dimensional glimpses $\left(v^{(t)}\right)$.
A glimpse corresponds to a three-dimensional hyperrectangular volume of video defined by location and scale parameters $\theta^{(t)}$, resampled to a representation of constant size much smaller than that of the video.
The resampling process is described in \ref{sec:resampling}.

Each resampled glimpse $v^{(t)}$ is passed through a convolutional network, concatenated with its parameters $\theta^{(t)}$ and fed to a fully-connected network to obtain the input $x^{(t)}$ to the next step of the recurrent network.

We use a two-layer recurrent neural network with Gated Recurrent Units\cite{Cho2014} to combine the information from previous glimpses and determine the location and scale of the next glimpse.

Figure \ref{fig:model} shows a schematic view of the model.

\subsection{Resampling}
\label{sec:resampling}

We employ a differential cropping operator similar to that used in DRAW \cite{draw}.
The intensity of each pixel in the glimpse is a linear combination of the intensities of all the pixels in the video, with coefficients determined by Gaussian kernels.
The kernel covariance is constrained to be diagonal, which allows to resample across each of the temporal, vertical and horizontal data dimensions independently.
In the one-dimensional case, glimpse pixels $v_i$ relate to video pixels $V_i$ by

\begin{equation}
\begin{split}
v_i &= \sum_{\tilde{i}} \phi(\delta,\sigma) V_{\tilde{i}} \\
\delta & =  \frac{i - m/2}{s} + l - \tilde{i} \\
\sigma & =  \frac{\alpha}{\min(s, 1)}
\end{split}
\end{equation}

where $i$ is a glimpse pixel index, $\tilde{i}$ is a video pixel index, $m$ is the size of the glimpse in pixels, and $l$ and $s$ are the location and scale of the glimpse, respectively.
$\phi( \delta, \sigma ) = \frac{1}{\sqrt{2 \pi}\sigma} \exp{- \frac{1}{2} \frac{\delta^2}{\sigma^2}}$ is the Gaussian filter.

Note that the kernel stride, standard deviation and intensity scaling factor are determined by a single scale parameter $s$.
Disconnecting these parameters as is done in \cite{draw} would give the model the freedom to introduce severe resampling artifacts such as aliasing, unnecessary blurring and noise.
The influence of $s$ on $\sigma$ is bounded to avoid too narrow kernels when zooming in beyond the native resolution of the input.
The hyperparameter $\alpha$ trades off aliasing and blurring; by visual inspection of the quality of the patches we chose to set it to $0.5$.

\subsection{Kernel truncation}

We further depart from \cite{draw} by truncating the Gaussian kernels at three deviations from their means.
In practice we implement this by projecting the corners of the glimpse into the space of video indices and cropping a box around them with the desired margin.
The outermost kernels are thus more truncated than the interior ones, and the pixels in the center of the glimpse are virtually untruncated.
The resulting equations are still differentiable, with the gradient providing information on where to move inside the box.
We have found this truncation to have little effect on training, while allowing significant computational savings.

\begin{figure}
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=.5in,
                    semithick]
  \path   ( 0, 0) node (state00) {$h^{(0)}_0$}
         +( 0, 3) node (state01) {$h^{(0)}_1$}
        ++( 3, 0) node (state10) {$h^{(1)}_0$}
         +( 0, 3) node (state11) {$h^{(1)}_1$}
        ++( 3, 0) node (state20) {$h^{(2)}_0$}
         +( 0, 3) node (state21) {$h^{(2)}_1$}
        ++( 3, 0) node (state30) {$h^{(3)}_0$}
         +( 0, 3) node (state31) {$h^{(3)}_1$}
        ++( 3, 0) node (state40) {$h^{(4)}_0$}
         +( 0, 3) node (state41) {$h^{(4)}_1$};

  \node (theta1)   [below right of=state01]{$\theta^{(1)}$};
  \node (theta2)   [below right of=state11]{$\theta^{(2)}$};
  \node (theta3)   [below right of=state21]{$\theta^{(3)}$};
  \node (theta4)   [below right of=state31]{$\theta^{(4)}$};

  \node (glimpse1) [right of=theta1]{$v^{(1)}$};
  \node (glimpse2) [right of=theta2]{$v^{(2)}$};
  \node (glimpse3) [right of=theta3]{$v^{(3)}$};
  \node (glimpse4) [right of=theta4]{$v^{(4)}$};
  
  \node (input1)   [below of=glimpse1]{$x^{(1)}$};
  \node (input2)   [below of=glimpse2]{$x^{(2)}$};
  \node (input3)   [below of=glimpse3]{$x^{(3)}$};
  \node (input4)   [below of=glimpse4]{$x^{(4)}$};

  \node (yhat1)    [below right of=state10]{$\hat{y}^{(1)}$};
  \node (yhat2)    [below right of=state20]{$\hat{y}^{(2)}$};
  \node (yhat3)    [below right of=state30]{$\hat{y}^{(3)}$};
  \node (yhat4)    [below right of=state40]{$\hat{y}^{(4)}$};
  
  \node (image)    [above of=state11]{$V$};

  \path (state00)  edge (state10) edge (state01)
        (state10)  edge (state20) edge (state11) edge (yhat1)
        (state20)  edge (state30) edge (state21) edge (yhat2)
        (state30)  edge (state40) edge (state31) edge (yhat3)
        (state40)                 edge (state41) edge (yhat4)

        (state01)  edge (state11)
        (state11)  edge (state21) edge (theta2)
        (state21)  edge (state31) edge (theta3)
        (state31)  edge (state41) edge (theta4)
        (state41)
        
        (theta1)   edge (glimpse1) edge (input1)
        (theta2)   edge (glimpse2) edge (input2)
        (theta3)   edge (glimpse3) edge (input3)
        (theta4)   edge (glimpse4) edge (input4)

        (glimpse1) edge (input1)
        (glimpse2) edge (input2)
        (glimpse3) edge (input3)
        (glimpse4) edge (input4)

        (input1)   edge (state10)
        (input2)   edge (state20)
        (input3)   edge (state30)
        (input4)   edge (state40)

        (image)    edge [bend right] (glimpse1)
                   edge [bend  left] (glimpse2)
                   edge [bend  left] (glimpse3)
                   edge [bend  left] (glimpse4);
\end{tikzpicture}
\label{fig:model}
\end{figure}

\section{Dataset}

We introduce a new fine-grained video classification dataset derived from the Cluttered MNIST dataset \cite{clutteredmnist}.
The examples consist of MNIST digits placed on a larger canvas in the presence of clutter.
Both the digit and the clutter move across the canvas over time according to identical motion models, in which we uniformly sample a time $t$ and a location $x$ for the object in the interior of the video and extrapolate a linear trajectory based on a uniformly chosen velocity.

Additionally, we partially occlude the digit by superimposing black bars perpendicular to its direction of travel.
This ensures that no one frame displays the entire digit, and thus solving the task requires aggregation of information over time.

With this dataset we intend to bridge the gap between datasets such as KTH \cite{kth} and UCF101 \cite{ucf101} that have too few examples for the kinds of architectures we are interested in and large datasets like Sports-1M \cite{sports1m} and LSMDC \cite{lsmdc} that are too expensive to experiment on.

\subsubsection*{Acknowledgments}

WRITEME

\bibliography{iclr2016_conference}
\bibliographystyle{iclr2016_conference}

\end{document}
